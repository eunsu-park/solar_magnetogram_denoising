{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_id = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_visible_devices(gpus[gpu_id], 'GPU')\n",
    "tf.config.experimental.set_memory_growth(gpus[gpu_id], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_display = 2000\n",
    "iter_save = 10000\n",
    "iter_max = 500000\n",
    "\n",
    "name_input = 'center'\n",
    "name_output = 'stacks'\n",
    "\n",
    "root_data = '/home/park_e/datasets'\n",
    "root_save = '/userhome/park_e/denoising_tf2'\n",
    "\n",
    "ch_input = 1\n",
    "ch_output = 1\n",
    "\n",
    "lim_hmi = 100.\n",
    "\n",
    "isize=256\n",
    "bsize = 1\n",
    "layer_max_d = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 'unet_cgan%d'%(layer_max_d)\n",
    "\n",
    "root_model = '%s/%s/model' % (root_save, mode)\n",
    "root_validation = '%s/%s/validation' % (root_save, mode)\n",
    "root_test = '%s/%s/test' % (root_save, mode)\n",
    "\n",
    "import os\n",
    "os.makedirs(root_model, exist_ok=True)\n",
    "os.makedirs(root_validation, exist_ok=True)\n",
    "os.makedirs(root_test, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "list_train_input = sorted(glob('%s/train/hmi/%s/*.npy'%(root_data, name_input)))\n",
    "list_train_output = sorted(glob('%s/train/hmi/%s/*.npy'%(root_data, name_output)))\n",
    "assert len(list_train_input) == len(list_train_output)\n",
    "list_train = list(zip(list_train_input, list_train_output))\n",
    "nb_train = len(list_train)\n",
    "\n",
    "list_validation_input = sorted(glob('%s/validation/hmi/%s/*.npy'%(root_data, name_input)))\n",
    "list_validation_output = sorted(glob('%s/validation/hmi/%s/*.npy'%(root_data, name_output)))\n",
    "assert len(list_validation_input) == len(list_validation_output)\n",
    "list_validation = list(zip(list_validation_input, list_validation_output))\n",
    "nb_validation = len(list_validation)\n",
    "\n",
    "\n",
    "list_test_input = sorted(glob('%s/test/hmi/%s/*.npy'%(root_data, name_input)))\n",
    "list_test_output = sorted(glob('%s/test/hmi/%s/*.npy'%(root_data, name_output)))\n",
    "assert len(list_test_input) == len(list_test_output)\n",
    "list_test = list(zip(list_test_input, list_test_output))\n",
    "nb_test = len(list_test)\n",
    "\n",
    "print(nb_train, nb_validation, nb_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils import bytescale\n",
    "from imageio import imsave\n",
    "\n",
    "def make_tensor(file_):\n",
    "    x = np.load(file_)[None, :, :, None].clip(-lim_hmi,lim_hmi)/lim_hmi\n",
    "    return x\n",
    "\n",
    "def make_output(gen_, tar_):\n",
    "    x = gen_.reshape(isize, isize)*lim_hmi\n",
    "    x_png = np.hstack((bytescale(x, imin=-30, imax=30),\n",
    "                       bytescale(x, imin=-100, imax=100)))\n",
    "    y = np.load(tar_)\n",
    "    y_png = np.hstack((bytescale(y, imin=-30, imax=30),\n",
    "                       bytescale(y, imin=-100, imax=100)))\n",
    "    \n",
    "    df_png = np.hstack((bytescale(x.clip(-30, 30)-y.clip(-30, 30), imin=-30, imax=30),\n",
    "                       bytescale(x.clip(-100, 100)-y.clip(-100, 100), imin=-100, imax=100)))\n",
    "\n",
    "    xy_png = np.vstack((x_png, y_png, df_png))\n",
    "    \n",
    "    return x, xy_png   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "\n",
    "def train_batch_generator():\n",
    "    epoch = i = 0\n",
    "    tmpsize = None\n",
    "    while True:\n",
    "        size = tmpsize if tmpsize else bsize\n",
    "        if i + size > nb_train :\n",
    "            shuffle(list_train)\n",
    "            i = 0\n",
    "            epoch += 1\n",
    "        batch_A = np.concatenate([make_tensor(list_train[j][0]) for j in range(i, i+size)], 0)\n",
    "        batch_B = np.concatenate([make_tensor(list_train[j][1]) for j in range(i, i+size)], 0)\n",
    "        i += size\n",
    "        tmpsize = yield epoch, batch_A, batch_B\n",
    "\n",
    "train_batch = train_batch_generator()\n",
    "\n",
    "#for n in range(10):\n",
    "#    A, B, C = next(train_batch)\n",
    "#    print(B.shape, C.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networks_tf2 import unet_generator, patch_discriminator\n",
    "network_G = unet_generator(isize, ch_input, ch_output)\n",
    "network_D = patch_discriminator(isize, ch_input, ch_output, layer_max_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_object(target, output):\n",
    "    return -tf.math.reduce_mean(tf.math.log(output+1e-12)*target+tf.math.log(1-output+1e-12)*(1-target))\n",
    "#loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "def multi_loss_L(fake_B, real_B):\n",
    "\n",
    "    loss_L0 = tf.reduce_mean(tf.abs(real_B - fake_B))    \n",
    "    \n",
    "    fake_B = fake_B*lim_hmi\n",
    "    real_B = real_B*lim_hmi\n",
    "    \n",
    "    loss_L1 = tf.reduce_mean(tf.abs(tf.clip_by_value(real_B, -100, 100)/100 - tf.clip_by_value(fake_B, -100, 100)/100))\n",
    "    loss_L2 = tf.reduce_mean(tf.abs(tf.clip_by_value(real_B, -30, 30)/30 - tf.clip_by_value(fake_B, -30, 30)/30))\n",
    "\n",
    "    return loss_L0, loss_L1, loss_L2\n",
    "\n",
    "def loss_Dis(output_D_real, output_D_fake):\n",
    "    loss_D_real = loss_object(target=tf.ones_like(output_D_real), output=output_D_real)\n",
    "    loss_D_fake = loss_object(target=tf.zeros_like(output_D_fake), output=output_D_fake)\n",
    "    return (loss_D_real+loss_D_fake)/2.\n",
    "\n",
    "def loss_Gen(output_D_fake, fake_B, real_B):\n",
    "    loss_G_fake = loss_object(target=tf.ones_like(output_D_fake), output=output_D_fake)\n",
    "    loss_L = tf.reduce_mean(tf.abs(real_B - fake_B))\n",
    "#    loss_L0, loss_L1, loss_L2 = multi_loss_L(real_B, fake_B)\n",
    "#    loss_L = loss_L0 + loss_L1 + loss_L2\n",
    "    \n",
    "    return loss_G_fake, loss_L\n",
    "\n",
    "optimizer_D = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "optimizer_G = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(real_A, real_B):\n",
    "    with tf.GradientTape() as tape_G, tf.GradientTape() as tape_D:\n",
    "        fake_B = network_G(real_A, training=True)\n",
    "        output_D_real = network_D([real_A, real_B], training=True)\n",
    "        output_D_fake = network_D([real_A, fake_B], training=True)\n",
    "\n",
    "        loss_G_fake, loss_L = loss_Gen(output_D_fake, fake_B, real_B)\n",
    "        loss_D = loss_Dis(output_D_real, output_D_fake)\n",
    "        loss_G = loss_G_fake + loss_L*100.\n",
    "\n",
    "    gradient_G = tape_G.gradient(loss_G, network_G.trainable_variables)\n",
    "    gradient_D = tape_D.gradient(loss_D, network_D.trainable_variables)\n",
    "\n",
    "    optimizer_G.apply_gradients(zip(gradient_G, network_G.trainable_variables))\n",
    "    optimizer_D.apply_gradients(zip(gradient_D, network_D.trainable_variables))\n",
    "\n",
    "    return loss_D, loss_G_fake, loss_L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "TM = time.localtime(time.time())\n",
    "t_now = '%04d-%02d-%02d %02d:%02d:%02d'%(TM.tm_year, TM.tm_mon, TM.tm_mday, TM.tm_hour, TM.tm_min, TM.tm_sec)\n",
    "\n",
    "print('\\n--------------------------------\\n')\n",
    "\n",
    "print('\\n%s : Now start below session!\\n'%(t_now))\n",
    "print('Model mode : %s'%mode)\n",
    "print('Data mode : %s to %s'%(name_input, name_output))\n",
    "print('Model save path: %s'%(root_model))\n",
    "print('Validation snap save path: %s'%(root_validation))\n",
    "print('Test result save path: %s'%(root_test))\n",
    "print('# of train, validation, and test datasets : %d, %d, %d'%(nb_train, nb_validation, nb_test))\n",
    "\n",
    "print('\\n--------------------------------\\n')\n",
    "\n",
    "t0 = time.time()\n",
    "epoch = iter_gen = 0\n",
    "err_D = err_D_sum = err_D_mean = 0\n",
    "err_G = err_G_sum = err_G_mean = 0\n",
    "err_L = err_L_sum = err_L_mean = 0\n",
    "\n",
    "while iter_gen <= iter_max :\n",
    "\n",
    "    epoch, train_A, train_B = next(train_batch)\n",
    "    train_A = tf.cast(train_A, tf.float32)\n",
    "    train_B = tf.cast(train_B, tf.float32)\n",
    "    \n",
    "    err_D, err_G, err_L = train_step(train_A, train_B)\n",
    "    \n",
    "    err_D_sum += err_D\n",
    "    err_G_sum += err_G\n",
    "    err_L_sum += err_L\n",
    "    \n",
    "    iter_gen += bsize\n",
    "    \n",
    "    if iter_gen % iter_display == 0:\n",
    "\n",
    "        err_D_mean = err_D_sum/iter_display\n",
    "        err_G_mean = err_G_sum/iter_display\n",
    "        err_L_mean = err_L_sum/iter_display\n",
    "        message1 = '[%d][%d/%d]' % (epoch, iter_gen, iter_max)\n",
    "        message2 = 'LOSS_D: %5.3f LOSS_G: %5.3f LOSS_L: %5.3f T: %dsec/%dits' % (err_D_mean, err_G_mean, err_L_mean, time.time()-t0, iter_display)\n",
    "        TM = time.localtime(time.time())\n",
    "        t_now = '%04d-%02d-%02d %02d:%02d:%02d'%(TM.tm_year, TM.tm_mon, TM.tm_mday, TM.tm_hour, TM.tm_min, TM.tm_sec)\n",
    "        print('%s: %s %s'%(t_now, message1, message2))\n",
    "\n",
    "        err_L_sum, err_G_sum, err_D_sum = 0, 0, 0\n",
    "        t0 = time.time()\n",
    "\n",
    "    if iter_gen % iter_save == 0:\n",
    "        \n",
    "        dst_model = '%s/%s.%07d'%(root_model, mode, iter_gen)\n",
    "        network_G.save('%s.G.h5'%(dst_model))\n",
    "        network_D.save('%s.D.h5'%(dst_model))\n",
    "        message3 = 'network_G and network_D are saved under %s'%(root_model)\n",
    "        TM = time.localtime(time.time())\n",
    "        t_now = '%04d-%02d-%02d %02d:%02d:%02d'%(TM.tm_year, TM.tm_mon, TM.tm_mday, TM.tm_hour, TM.tm_min, TM.tm_sec)\n",
    "        print('%s: %s'%(t_now, message3))\n",
    "\n",
    "        path_validation = '%s/iter_%07d'%(root_validation, iter_gen)\n",
    "        os.makedirs(path_validation, exist_ok=True)\n",
    "        for pair_ in list_validation :\n",
    "            file_A, file_B = pair_\n",
    "            real_A = make_tensor(file_A)\n",
    "            real_A = tf.cast(real_A, tf.float32)\n",
    "            fake_B = network_G.predict(real_A)\n",
    "            fake_B, fake_B_png = make_output(fake_B, file_B)\n",
    "            name_save = '%s.%07d.%s'%(mode, iter_gen, file_A.split('/')[-1][-23:-4])\n",
    "            np.save('%s/%s.npy'%(path_validation, name_save), fake_B)\n",
    "            imsave('%s/%s.png'%(path_validation, name_save), fake_B_png)\n",
    "        message4 = 'Validation snaps (%d images) are saved in %s'%(nb_validation, path_validation)\n",
    "        TM = time.localtime(time.time())\n",
    "        t_now = '%04d-%02d-%02d %02d:%02d:%02d'%(TM.tm_year, TM.tm_mon, TM.tm_mday, TM.tm_hour, TM.tm_min, TM.tm_sec)\n",
    "        print('%s: %s'%(t_now, message4))\n",
    "        \n",
    "        path_test = '%s/iter_%07d'%(root_test, iter_gen)\n",
    "        os.makedirs(path_test, exist_ok=True)\n",
    "        for pair_ in list_test :\n",
    "            file_A, file_B = pair_\n",
    "            real_A = make_tensor(file_A)\n",
    "            real_A = tf.cast(real_A, tf.float32)\n",
    "            fake_B = network_G.predict(real_A)\n",
    "            fake_B, fake_B_png = make_output(fake_B, file_B)\n",
    "            name_save = '%s.%07d.%s'%(mode, iter_gen, file_A.split('/')[-1][-23:-4])\n",
    "            np.save('%s/%s.npy'%(path_test, name_save), fake_B)\n",
    "            imsave('%s/%s.png'%(path_test, name_save), fake_B_png)\n",
    "        message5 = 'Test snaps (%d images) are saved in %s'%(nb_test, path_test)\n",
    "        TM = time.localtime(time.time())\n",
    "        t_now = '%04d-%02d-%02d %02d:%02d:%02d'%(TM.tm_year, TM.tm_mon, TM.tm_mday, TM.tm_hour, TM.tm_min, TM.tm_sec)\n",
    "        print('%s: %s'%(t_now, message5))        \n",
    "        \n",
    "        t0 = time.time()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
