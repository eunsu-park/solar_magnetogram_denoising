{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_id       = 1\n",
    "iter_display = 20000\n",
    "iter_save    = 100000\n",
    "iter_max     = 1000000\n",
    "ch_input     = 1       # Number of input channels\n",
    "ch_output    = 1       # Number of output channels\n",
    "lim_hmi      = 3000.   # HMI data range\n",
    "isize        = 256     # Image size\n",
    "bsize        = 16      # Batch size\n",
    "use_fm_loss  = False   # Feature mapping loss, default=False\n",
    "use_l1_loss  = True    # L1 loss, default = True\n",
    "\n",
    "root_data    = 'path_to_data'\n",
    "root_save    = 'path_to_save'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_model      = '%s/model' % (root_save)\n",
    "root_validation = '%s/validation' % (root_save)\n",
    "root_test       = '%s/test' % (root_save)\n",
    "\n",
    "import os\n",
    "os.makedirs(root_model, exist_ok=True)\n",
    "os.makedirs(root_validation, exist_ok=True)\n",
    "os.makedirs(root_test, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_visible_devices(gpus[gpu_id], 'GPU')\n",
    "tf.config.experimental.set_memory_growth(gpus[gpu_id], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" data tree\n",
    "\n",
    "root_data - train - center (input)\n",
    "                  - stacks (target)\n",
    "\n",
    "          - validation - center\n",
    "                       - stacks\n",
    "\n",
    "          - test - center\n",
    "                 - stacks\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "list_train_input  = sorted(glob('%s/train/center/*.npy'%(root_data)))\n",
    "list_train_output = sorted(glob('%s/train/stacks/*.npy'%(root_data)))\n",
    "list_train        = list(zip(list_train_input, list_train_output))\n",
    "nb_train          = len(list_train)\n",
    "\n",
    "list_validation   = sorted(glob('%s/validation/center/*.npy'%(root_data)))\n",
    "nb_validation     = len(list_validation)\n",
    "\n",
    "list_test         = sorted(glob('%s/test/center/*.npy'%(root_data)))\n",
    "nb_test           = len(list_test)\n",
    "\n",
    "print(nb_train, nb_validation, nb_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils_data import bytescale\n",
    "from random import shuffle\n",
    "\n",
    "def make_tensor(file_):\n",
    "    x = np.load(file_)[None, :, :, None]/lim_hmi\n",
    "    return x\n",
    "\n",
    "def make_output(gen_):\n",
    "    x = gen_.numpy().reshape(isize, isize)*lim_hmi\n",
    "    x_png = bytescale(x, imin=-30, imax=30)\n",
    "    return x, x_png\n",
    "\n",
    "def train_batch_generator():\n",
    "    epoch = i = 0\n",
    "    size = bsize\n",
    "    while True:\n",
    "        if i + size > nb_train :\n",
    "            shuffle(list_train)\n",
    "            i = 0\n",
    "            epoch += 1\n",
    "        batch_A = np.concatenate([make_tensor(list_train[j][0]) for j in range(i, i+size)], 0)\n",
    "        batch_B = np.concatenate([make_tensor(list_train[j][1]) for j in range(i, i+size)], 0)\n",
    "        i += size\n",
    "        yield epoch, tf.cast(batch_A, tf.float32), tf.cast(batch_B, tf.float32)\n",
    "\n",
    "def check_train_batch_generator():\n",
    "    train_batch = train_batch_generator()\n",
    "    for n in range(5):\n",
    "        epoch, batch_A, batch_B = next(train_batch)\n",
    "        print(epoch, batch_A.shape, batch_B.shape)\n",
    "    \n",
    "#check_train_batch_generator() to check train_batch_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networks import generator, discriminator\n",
    "network_G = generator(isize, ch_input, ch_output)\n",
    "network_D = discriminator(isize, ch_input, ch_output)\n",
    "\n",
    "network_G.summary()\n",
    "network_D.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(target, output):\n",
    "    return -tf.math.reduce_mean(tf.math.log(output+1e-12)*target+tf.math.log(1-output+1e-12)*(1-target))\n",
    "\n",
    "#lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(1e-4,decay_steps=1024,decay_rate=0.96)\n",
    "optimizer_D = tf.keras.optimizers.Adam(2.0e-4, beta_1=0.5)\n",
    "optimizer_G = tf.keras.optimizers.Adam(2.0e-4, beta_1=0.5)\n",
    "\n",
    "@tf.function\n",
    "def train_step(real_A, real_B, fm_weight=10, l1_weight=100):\n",
    "    with tf.GradientTape() as tape_G, tf.GradientTape() as tape_D:\n",
    "\n",
    "        fake_B = network_G(real_A, training=True)\n",
    "        \n",
    "        output_D_real = network_D([real_A, real_B], training=True)\n",
    "        output_D_fake = network_D([real_A, fake_B], training=True)\n",
    "        \n",
    "        loss_D_real = loss_function(target=tf.ones_like(output_D_real[0]), output=output_D_real[0])\n",
    "        loss_D_fake = loss_function(target=tf.zeros_like(output_D_fake[0]), output=output_D_fake[0])\n",
    "        loss_D = (loss_D_real + loss_D_fake)/2.\n",
    "\n",
    "        loss_G_fake = loss_function(target=tf.ones_like(output_D_fake[0]), output=output_D_fake[0])\n",
    "        \n",
    "        feature_real = output_D_real[1:]\n",
    "        feature_fake = output_D_fake[1:]\n",
    "        \n",
    "        loss_F = 0\n",
    "        for i in range(len(feature_fake)):\n",
    "            loss_F += tf.math.reduce_mean(tf.abs(feature_fake[i]-feature_real[i]))\n",
    "        loss_F *= fm_weight\n",
    "        \n",
    "        loss_L = tf.reduce_mean(tf.abs(real_B - fake_B)) * l1_weight\n",
    "        \n",
    "        loss_G = loss_G_fake\n",
    "        if use_fm_loss :\n",
    "            loss_G += loss_G_feature\n",
    "        if use_l1_loss :\n",
    "            loss_G += loss_L\n",
    "        \n",
    "    gradient_G = tape_G.gradient(loss_G, network_G.trainable_variables)\n",
    "    gradient_D = tape_D.gradient(loss_D, network_D.trainable_variables)\n",
    "\n",
    "    optimizer_G.apply_gradients(zip(gradient_G, network_G.trainable_variables))\n",
    "    optimizer_D.apply_gradients(zip(gradient_D, network_D.trainable_variables))\n",
    "\n",
    "    return loss_D, loss_G_fake, loss_F, loss_L\n",
    "\n",
    "@tf.function\n",
    "def generation_step(real_A):\n",
    "    return network_G(tf.cast(real_A, tf.float32), training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def t_now():\n",
    "    TM = time.localtime(time.time())\n",
    "    return '%04d-%02d-%02d %02d:%02d:%02d'%(TM.tm_year, TM.tm_mon, TM.tm_mday, TM.tm_hour, TM.tm_min, TM.tm_sec)\n",
    "\n",
    "print('\\n------------------------------------ Summary ------------------------------------\\n')\n",
    "\n",
    "print('\\n%s: Now start below session!\\n'%(t_now()))\n",
    "print('Model save path: %s'%(root_model))\n",
    "print('Validation snap save path: %s'%(root_validation))\n",
    "print('Test result save path: %s'%(root_test))\n",
    "print('# of train, validation, and test datasets : %d, %d, %d'%(nb_train, nb_validation, nb_test))\n",
    "\n",
    "print('\\n---------------------------------------------------------------------------------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imageio import imsave\n",
    "\n",
    "train_batch = train_batch_generator()\n",
    "\n",
    "t0 = time.time()\n",
    "epoch = iter_gen = 0\n",
    "err_D = err_D_sum = err_D_mean = 0\n",
    "err_G = err_G_sum = err_G_mean = 0\n",
    "err_F = err_F_sum = err_F_mean = 0\n",
    "err_L = err_L_sum = err_L_mean = 0\n",
    "\n",
    "while iter_gen <= iter_max :\n",
    "\n",
    "    epoch, train_A, train_B = next(train_batch)\n",
    "    err_D, err_G, err_F, err_L = train_step(train_A, train_B)\n",
    "    \n",
    "    err_D_sum += err_D*bsize\n",
    "    err_G_sum += err_G*bsize\n",
    "    err_F_sum += err_F*bsize\n",
    "    err_L_sum += err_L*bsize\n",
    "    \n",
    "    iter_gen += bsize\n",
    "    \n",
    "    if iter_gen % iter_display == 0:\n",
    "\n",
    "        err_D_mean = err_D_sum/iter_display\n",
    "        err_G_mean = err_G_sum/iter_display\n",
    "        err_F_mean = err_F_sum/iter_display\n",
    "        err_L_mean = err_L_sum/iter_display\n",
    "        \n",
    "        message1 = '[%d][%d/%d]' % (epoch, iter_gen, iter_max)\n",
    "        message2 = 'Loss_D: %5.3f Loss_G: %5.3f Loss_F: %5.3f Loss_L: %5.3f T: %dsec/%dits'%(err_D_mean, err_G_mean, err_F_mean, err_L_mean, time.time()-t0, iter_display)\n",
    "        print('%s: %s %s'%(t_now(), message1, message2))\n",
    "\n",
    "        err_G_sum, err_D_sum, err_F_sum, err_L_sum = 0, 0, 0, 0\n",
    "        t0 = time.time()\n",
    "\n",
    "    if iter_gen % iter_save == 0:\n",
    "        \n",
    "        network_G.save('%s/denoising.gan.%07d.G.h5'%(root_model, iter_gen))\n",
    "        network_D.save('%s/denoising.gan.%07d.D.h5'%(root_model, iter_gen))\n",
    "        message3 = 'network_G and network_D are saved under %s'%(root_model)\n",
    "        print('%s: %s'%(t_now(), message3))\n",
    "        \n",
    "        path_validation = '%s/iter_%07d'%(root_validation, iter_gen)\n",
    "        path_test = '%s/iter_%07d'%(root_test, iter_gen)\n",
    "        os.makedirs(path_validation, exist_ok=True)\n",
    "        os.makedirs(path_test, exist_ok=True)\n",
    "        \n",
    "        for file_A in list_validation :\n",
    "            date = file_A.split('.')[-2]\n",
    "            fake_B = generation_step(make_tensor(file_A))\n",
    "            fake_B, fake_B_png = make_output(fake_B)\n",
    "            name = 'denoising.gan.%07d.%s'%(iter_gen, date)\n",
    "            np.save('%s/%s.npy'%(path_validation, name), fake_B)\n",
    "            imsave('%s/%s.png'%(path_validation, name), fake_B_png)\n",
    "\n",
    "        message4 = 'Validation snaps (%d images) are saved in %s'%(nb_validation, path_validation)\n",
    "        print('%s: %s'%(t_now(), message4))\n",
    "        \n",
    "        for file_A in list_test :\n",
    "            date = file_A.split('.')[-2]\n",
    "            fake_B = generation_step(make_tensor(file_A))\n",
    "            fake_B, fake_B_png = make_output(fake_B)\n",
    "            name = 'denoising.gan.%07d.%s'%(iter_gen, date)\n",
    "            np.save('%s/%s.npy'%(path_test, name), fake_B)\n",
    "            imsave('%s/%s.png'%(path_test, name), fake_B_png)\n",
    "\n",
    "        message5 = 'Test results (%d images) are saved in %s'%(nb_test, path_test)\n",
    "        print('%s: %s'%(t_now(), message5))\n",
    "        \n",
    "        t0 = time.time()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
