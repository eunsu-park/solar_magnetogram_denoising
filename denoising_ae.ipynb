{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_id       = 0\n",
    "iter_display = 20000\n",
    "iter_save    = 100000\n",
    "iter_max     = 1000000\n",
    "ch_input     = 1       # Number of input channels\n",
    "lim_hmi      = 3000.   # HMI data range\n",
    "isize        = 256     # Image size\n",
    "bsize        = 16      # Batch size\n",
    "loss         = 'mae'\n",
    "\n",
    "root_data    = 'path_to_data'\n",
    "root_save    = 'path_to_save'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_model      = '%s/model' % (root_save)\n",
    "root_validation = '%s/validation' % (root_save)\n",
    "root_test       = '%s/test' % (root_save)\n",
    "\n",
    "import os\n",
    "os.makedirs(root_model, exist_ok=True)\n",
    "os.makedirs(root_validation, exist_ok=True)\n",
    "os.makedirs(root_test, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_visible_devices(gpus[gpu_id], 'GPU')\n",
    "tf.config.experimental.set_memory_growth(gpus[gpu_id], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" data tree\n",
    "\n",
    "root_data - train - center (input, target)\n",
    "          - validation - center\n",
    "          - test - center\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "list_train        = sorted(glob('%s/train/center/*.npy'%(root_data)))\n",
    "nb_train          = len(list_train)\n",
    "\n",
    "list_validation   = sorted(glob('%s/validation/center/*.npy'%(root_data)))\n",
    "nb_validation     = len(list_validation)\n",
    "\n",
    "list_test         = sorted(glob('%s/test/center/*.npy'%(root_data)))\n",
    "nb_test           = len(list_test)\n",
    "\n",
    "print(nb_train, nb_validation, nb_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils_data import bytescale\n",
    "from random import shuffle\n",
    "\n",
    "def make_tensor(file_):\n",
    "    x = np.load(file_)[None, :, :, None]/lim_hmi\n",
    "    return x\n",
    "\n",
    "def make_output(gen_):\n",
    "    x = gen_.numpy().reshape(isize, isize)*lim_hmi\n",
    "    x_png = bytescale(x, imin=-30, imax=30)\n",
    "    return x, x_png\n",
    "\n",
    "def train_batch_generator():\n",
    "    epoch = i = 0\n",
    "    size = bsize\n",
    "    while True:\n",
    "        if i + size > nb_train :\n",
    "            shuffle(list_train)\n",
    "            i = 0\n",
    "            epoch += 1\n",
    "        batch_B = np.concatenate([make_tensor(list_train[j]) for j in range(i, i+size)], 0)\n",
    "        batch_A = batch_B + np.random.normal(0, 10./lim_hmi, batch_B.shape)\n",
    "        i += size\n",
    "        yield epoch, tf.cast(batch_A, tf.float32), tf.cast(batch_B, tf.float32)\n",
    "\n",
    "def check_train_batch_generator():\n",
    "    train_batch = train_batch_generator()\n",
    "    for n in range(5):\n",
    "        epoch, batch_A, batch_B = next(train_batch)\n",
    "        print(epoch, batch_A.shape, batch_B.shape)\n",
    "    \n",
    "#check_train_batch_generator()# to check train_batch_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networks import generator\n",
    "network_G = generator(isize, ch_input, ch_input)\n",
    "network_G.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(target, output):\n",
    "    return -tf.math.reduce_mean(tf.math.log(output+1e-12)*target+tf.math.log(1-output+1e-12)*(1-target))\n",
    "\n",
    "#lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(1e-4,decay_steps=1024,decay_rate=0.96)\n",
    "optimizer_G = tf.keras.optimizers.Adam(2.0e-4, beta_1=0.5)\n",
    "\n",
    "@tf.function\n",
    "def train_step(real_A, real_B, fm_weight=10, l1_weight=100):\n",
    "    with tf.GradientTape() as tape_G:\n",
    "\n",
    "        fake_B = network_G(real_A, training=True)\n",
    "        mae = tf.reduce_mean(tf.abs(real_B - fake_B))\n",
    "        mse = tf.reduce_mean(tf.square(real_B - fake_B))\n",
    "        if loss.lower() == 'mae' :\n",
    "            loss_G = mae\n",
    "        elif loss.lower() == 'mse' :\n",
    "            loss_G = mse\n",
    "        \n",
    "    gradient_G = tape_G.gradient(loss_G, network_G.trainable_variables)\n",
    "    optimizer_G.apply_gradients(zip(gradient_G, network_G.trainable_variables))\n",
    "\n",
    "    return loss_G, mae, mse\n",
    "\n",
    "@tf.function\n",
    "def generation_step(real_A):\n",
    "    return network_G(tf.cast(real_A, tf.float32), training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def t_now():\n",
    "    TM = time.localtime(time.time())\n",
    "    return '%04d-%02d-%02d %02d:%02d:%02d'%(TM.tm_year, TM.tm_mon, TM.tm_mday, TM.tm_hour, TM.tm_min, TM.tm_sec)\n",
    "\n",
    "print('\\n------------------------------------ Summary ------------------------------------\\n')\n",
    "\n",
    "print('\\n%s: Now start below session!\\n'%(t_now()))\n",
    "print('Model save path: %s'%(root_model))\n",
    "print('Validation snap save path: %s'%(root_validation))\n",
    "print('Test result save path: %s'%(root_test))\n",
    "print('# of train, validation, and test datasets : %d, %d, %d'%(nb_train, nb_validation, nb_test))\n",
    "\n",
    "print('\\n---------------------------------------------------------------------------------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imageio import imsave\n",
    "\n",
    "train_batch = train_batch_generator()\n",
    "\n",
    "t0 = time.time()\n",
    "epoch = iter_gen = 0\n",
    "err_G = err_G_sum = err_G_mean = 0\n",
    "mae = mae_sum = mae_mean = 0\n",
    "mse = mse_sum = mse_mean = 0\n",
    "\n",
    "while iter_gen <= iter_max :\n",
    "\n",
    "    epoch, train_A, train_B = next(train_batch)\n",
    "    err_G, mae, mse = train_step(train_A, train_B)\n",
    "    \n",
    "    err_G_sum += err_G*bsize\n",
    "    mae_sum += mae*bsize\n",
    "    mse_sum += mse*bsize\n",
    "    \n",
    "    iter_gen += bsize\n",
    "    \n",
    "    if iter_gen % iter_display == 0:\n",
    "\n",
    "        err_G_mean = err_G_sum/iter_display\n",
    "        mae_mean = mae_sum/iter_display\n",
    "        mse_mean = mse_sum/iter_display\n",
    "        \n",
    "        message1 = '[%d][%d/%d]' % (epoch, iter_gen, iter_max)\n",
    "        message2 = 'Loss: %5.3f MAE: %5.3f MSE: %5.3f T: %dsec/%dits'%(err_G_mean, mae_mean, mse_mean, time.time()-t0, iter_display)\n",
    "        print('%s: %s %s'%(t_now(), message1, message2))\n",
    "\n",
    "        err_G = err_G_sum = err_G_mean = 0\n",
    "        mae = mae_sum = mae_mean = 0\n",
    "        mse = mse_sum = mse_mean = 0\n",
    "        t0 = time.time()\n",
    "\n",
    "    if iter_gen % iter_save == 0:\n",
    "        \n",
    "        network_G.save('%s/denoising.ae.%07d.G.h5'%(root_model, iter_gen))\n",
    "        message3 = 'network_G is saved under %s'%(root_model)\n",
    "        print('%s: %s'%(t_now(), message3))\n",
    "        \n",
    "        path_validation = '%s/iter_%07d'%(root_validation, iter_gen)\n",
    "        path_test = '%s/iter_%07d'%(root_test, iter_gen)\n",
    "        os.makedirs(path_validation, exist_ok=True)\n",
    "        os.makedirs(path_test, exist_ok=True)\n",
    "        \n",
    "        for file_A in list_validation :\n",
    "            date = file_A.split('.')[-2]\n",
    "            fake_B = generation_step(make_tensor(file_A))\n",
    "            fake_B, fake_B_png = make_output(fake_B)\n",
    "            name = 'denoising.ae.%07d.%s'%(iter_gen, date)\n",
    "            np.save('%s/%s.npy'%(path_validation, name), fake_B)\n",
    "            imsave('%s/%s.png'%(path_validation, name), fake_B_png)\n",
    "\n",
    "        message4 = 'Validation snaps (%d images) are saved in %s'%(nb_validation, path_validation)\n",
    "        print('%s: %s'%(t_now(), message4))\n",
    "        \n",
    "        for file_A in list_test :\n",
    "            date = file_A.split('.')[-2]\n",
    "            fake_B = generation_step(make_tensor(file_A))\n",
    "            fake_B, fake_B_png = make_output(fake_B)\n",
    "            name = 'denoising.ae.%07d.%s'%(iter_gen, date)\n",
    "            np.save('%s/%s.npy'%(path_test, name), fake_B)\n",
    "            imsave('%s/%s.png'%(path_test, name), fake_B_png)\n",
    "\n",
    "        message5 = 'Test results (%d images) are saved in %s'%(nb_test, path_test)\n",
    "        print('%s: %s'%(t_now(), message5))\n",
    "        \n",
    "        t0 = time.time()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
